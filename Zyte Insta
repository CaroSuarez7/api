import requests
from bs4 import BeautifulSoup
import time

# Your Zyte Smart Proxy Manager API key here
ZYTE_SPM_API_KEY = 'YOUR_ZYTE_API_KEY'

# Proxy endpoint with authentication
ZYTE_PROXY = f'http://apikey:{ZYTE_SPM_API_KEY}@proxy.zyte.com:8011/'

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/115.0.0.0 Safari/537.36"
}

def zyte_get(url):
    """Fetch a URL through Zyte Smart Proxy with retries."""
    for _ in range(5):
        try:
            response = requests.get(url, proxies={"http": ZYTE_PROXY, "https": ZYTE_PROXY}, headers=HEADERS, timeout=10)
            if response.status_code == 200:
                return response.text
            else:
                print(f"Warning: Status code {response.status_code} for URL: {url}")
        except Exception as e:
            print(f"Error fetching URL {url}: {e}")
        time.sleep(2)
    return None

def get_likers_usernames(post_shortcode):
    """
    Scrape Instagram post likers usernames by shortcode.
    Note: Instagram limits access to likers; this may fail or be incomplete without login.
    """
    post_url = f"https://www.instagram.com/p/{post_shortcode}/"
    html = zyte_get(post_url)
    if not html:
        print("Failed to fetch post page")
        return []

    soup = BeautifulSoup(html, 'html.parser')

    # Attempt to extract likers from shared data JSON embedded in the page
    scripts = soup.find_all('script', type='text/javascript')
    shared_data = None
    for script in scripts:
        if script.string and 'window._sharedData' in script.string:
            json_text = script.string.strip().replace('window._sharedData =', '').rstrip(';')
            import json
            try:
                shared_data = json.loads(json_text)
                break
            except Exception as e:
                print("Failed to parse sharedData JSON:", e)

    if not shared_data:
        print("No sharedData found")
        return []

    try:
        # Navigate JSON to get likers list (this may be limited)
        edges = shared_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_liked_by']['edges']
        usernames = [edge['node']['username'] for edge in edges]
        return usernames
    except Exception as e:
        print("Error extracting likers usernames:", e)
        return []

def scrape_user_profile(username):
    """Scrape user profile bio and recent post captions."""
    profile_url = f"https://www.instagram.com/{username}/"
    html = zyte_get(profile_url)
    if not html:
        print(f"Failed to fetch profile for {username}")
        return None

    soup = BeautifulSoup(html, 'html.parser')

    # Extract bio
    bio = ""
    meta = soup.find("meta", property="og:description")
    if meta and meta.get("content"):
        bio = meta["content"].split("-")[0].strip()  # Usually bio text

    # Extract recent post captions
    captions = []
    scripts = soup.find_all('script', type='text/javascript')
    shared_data = None
    for script in scripts:
        if script.string and 'window._sharedData' in script.string:
            json_text = script.string.strip().replace('window._sharedData =', '').rstrip(';')
            import json
            try:
                shared_data = json.loads(json_text)
                break
            except:
                pass

    if shared_data:
        try:
            edges = shared_data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['edges']
            for edge in edges[:5]:  # limit to 5 recent posts
                node = edge['node']
                caption_edges = node.get('edge_media_to_caption', {}).get('edges', [])
                if caption_edges:
                    captions.append(caption_edges[0]['node']['text'])
        except:
            pass

    return {
        "username": username,
        "bio": bio,
        "recent_captions": captions
    }

if __name__ == "__main__":
    post_shortcode = "DKPdvBoR813"  # Replace with your Instagram post shortcode

    print(f"Scraping likers for post shortcode: {post_shortcode}")
    likers = get_likers_usernames(post_shortcode)
    print(f"Found {len(likers)} likers (may be partial):", likers)

    for username in likers:
        profile_data = scrape_user_profile(username)
        if profile_data:
            print(f"User: {profile_data['username']}")
            print(f"Bio: {profile_data['bio']}")
            print(f"Recent Captions: {profile_data['recent_captions']}")
            print("-" * 40)
        time.sleep(1)  # Be kind to Instagram servers!
